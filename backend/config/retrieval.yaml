policy_terms:
  - grade
  - grading
  - policy
  - policies
  - regulation
  - regulations
  - requirement
  - requirements
  - academic standards
  - degree requirements
  - gre
  - gmat
  - english proficiency
  - toefl
  - ielts
  - duolingo
  - transfer credit
  - add/drop
  - withdrawal
  - leave of absence
  - pass/fail
  - probation
  - dismissal

tier_boosts:
  0: 3.0
  1: 1.5
  2: 1.2
  3: 1.0
  4: 1.0

intent:
  course_keywords: ["course descriptions", "show courses", "list courses", "prerequisite", "prerequisites", "syllabus"]
  degree_keywords:
    - ms
    - m.s.
    - phd
    - ph.d
    - mba
    - mfa
    - ma
    - msc
    - program requirements
    - curriculum
    - concentration
    - how many credits
    - credit requirement
    - admissions
    - gre
    - gpa
  course_code_regex: "\\b[A-Z]{3,5}\\s?\\d{3}[A-Z]?\\b"

nudges:
  policy_acadreg_url: 1.07
  same_program_bonus: 2.00
  course_url_bonus: 1.80
  course_title_bonus: 1.40
  other_program_tier34_penalty: 0.25

guarantees:
  ensure_tier1_on_policy: true
  ensure_tier4_on_program: true

tier4_gate:
  use_embedding: true
  min_title_sim: 0.35
  min_alt_sim: 0.32

retrieval_sizes:
  topn_default: 120
  k: 5

# chunking configuration
chunking:
  enable_contextual_headers: true     # prepend section hierarchy to chunks
  enable_overlap: true                # use overlapping chunks
  text_chunk_size: 2                  # number of text items per chunk
  text_overlap: 1                     # overlap for text chunks
  list_chunk_size: 5                  # number of list items per chunk
  list_overlap: 2                     # overlap for list chunks

followups:
  hints:
    - "for "
    - "now "
    - "do it for"
    - "for the"
    - "make that for"
    - "do that for"
    - "do it"
    - "that"
    - "this"
    - "same"

# beam search configuration for answer generation
beam_search:
  enabled: false          
  beam_width: 5           # number of beams to explore
  num_candidates: 3       # number of candidate answers to generate and score
  diversity_penalty: 0.3  # encourage diverse outputs (0.0 = no diversity, 1.0 = max diversity)

gold_set:
  # enable/disable gold set features
  enabled: true  # Disabled to test actual LLM performance
  
  # path to gold JSONL file (relative to backend/)
  gold_file_path: "../automation_testing/gold.jsonl"
  
  # direct answer settings
  enable_direct_answer: true
  direct_answer_threshold: 0.95  # 95% similarity for direct answers

enhancements:
  enabled: false  # master switch for all enhancements - DISABLED: hurts scores by 23%
  
  # query Enhancement
  query_enhancement:
    enabled: false
    expand_acronyms: true
    boost_key_terms: true
    policy_rewriting: true
  
  # re-ranking
  reranking:
    enabled: false
    use_cross_encoder: true  # cross-encoder/ms-marco-MiniLM-L-6-v2
    use_tfidf: true
    use_metadata: true
    max_candidates_for_cross_encoder: 20
    diversity_threshold: 0.7
    
    # score combination weights (should sum to 1.0)
    weights:
      semantic: 0.25      
      cross_encoder: 0.45 
      tfidf: 0.15         
      metadata: 0.15      
  
  # contextual Compression (rule-based)
  compression:
    enabled: false
    deduplicate: true
    aggressive: false
    min_chunk_length: 200
    compression_ratio:
      tier1: 0.8
      tier2: 0.7
      tier3: 0.6
      tier4: 0.5

performance:
  lazy_load_models: true
  cache_size: 128
  max_tokens: 200  # Increased from 128 - allow more complete answers with all nuggets
  use_finetuned_model: false  # Set to true to use fine-tuned model, false for base model
  
  gold_chunk_boost: 2.5
  gold_url_boost: 1.5
  gold_answer_boost: 1.8
  
  # ensure at least one gold chunk in results if relevant
  ensure_gold_in_results: true
  min_gold_score: 0.3
  
  # prefer gold answers when model is uncertain
  prefer_gold_on_uncertainty: true
  
  # semantic matching settings
  semantic_match:
    enabled: true
    threshold: 0.85

# Synthetic Q&A Generation - improves semantic matching without gold set
synthetic_qa:
  enabled: false  # Generate Q&A pairs from catalog chunks using LLM
  boost_synthetic_qa: 1.3  # Slight boost for synthetic Q&A chunks (they match queries better)
  min_chunk_length: 50  # Skip chunks shorter than this
  question_model: "google/flan-t5-large" # Model for question generation (only runs during indexing, not live)
  temperature: 0.5  # Lower = more focused questions
  max_question_length: 60  # Max tokens for generated questions
  force_cpu: true  # Force CPU for generation (set to false to try GPU)
  
  # Tier filtering - which tiers to generate synthetic Q&A for
  # Options: "all" or list of tier numbers [0, 1, 2, 3, 4]
  # Tier 0: Gold set, Tier 1: Academic regs, Tier 2: General, Tier 3: Program-specific, Tier 4: Other programs
  # Example: [1, 2] = only academic regulations and general info (~2000 chunks = ~1hr on CPU)
  # Example: "all" = all chunks (~11000 chunks = ~6-15hrs on CPU)
  generate_for_tiers: [1, 2]  # Only generate for important chunks to save time

# gold set statistics
stats:
  total_entries: 0
  categories: {}
