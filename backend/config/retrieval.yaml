policy_terms:
  - grade
  - grading
  - policy
  - policies
  - regulation
  - regulations
  - requirement
  - requirements
  - academic standards
  - degree requirements
  - gre
  - gmat
  - english proficiency
  - toefl
  - ielts
  - duolingo
  - transfer credit
  - add/drop
  - withdrawal
  - leave of absence
  - pass/fail
  - probation
  - dismissal

# Tier system configuration
tier_system:
  enabled: false  # Set to false to disable tier boosting (all chunks treated equally)

tier_boosts:
  0: 3.0
  1: 1.5
  2: 1.2
  3: 1.0
  4: 1.0

intent:
  course_keywords: ["course descriptions", "show courses", "list courses", "prerequisite", "prerequisites", "syllabus"]
  degree_keywords:
    - ms
    - m.s.
    - phd
    - ph.d
    - mba
    - mfa
    - ma
    - msc
    - program requirements
    - curriculum
    - concentration
    - how many credits
    - credit requirement
    - admissions
    - gre
    - gpa
  course_code_regex: "\\b[A-Z]{3,5}\\s?\\d{3}[A-Z]?\\b"

nudges:
  policy_acadreg_url: 1.07
  same_program_bonus: 2.00
  course_url_bonus: 1.80
  course_title_bonus: 1.40
  other_program_tier34_penalty: 0.25

guarantees:
  ensure_tier1_on_policy: true
  ensure_tier4_on_program: true

tier4_gate:
  use_embedding: true
  min_title_sim: 0.35
  min_alt_sim: 0.32

retrieval_sizes:
  topn_default: 120
  k: 2  # Number of chunks sent to LLM

# Query transformation configuration
query_transformation:
  enabled: false

# chunking configuration
chunking:
  enable_contextual_headers: true     # prepend section hierarchy to chunks
  enable_overlap: false               # disable overlapping (not useful with size=1)
  text_chunk_size: 1                  # number of text items per chunk (reduced for more focused Q&A)
  text_overlap: 0                     # overlap for text chunks (no overlap needed with size=1)
  list_chunk_size: 3                  # number of list items per chunk (reduced for more focused Q&A)
  list_overlap: 0                     # no overlap needed with small chunks

followups:
  hints:
    - "for "
    - "now "
    - "do it for"
    - "for the"
    - "make that for"
    - "do that for"
    - "do it"
    - "that"
    - "this"
    - "same"

# Calendar linking configuration
calendar_linking:
  enabled: false  # Set to false to disable automatic calendar fallback links

# beam search configuration for answer generation
beam_search:
  enabled: false          
  beam_width: 5           # number of beams to explore
  num_candidates: 3       # number of candidate answers to generate and score
  diversity_penalty: 0.3  # encourage diverse outputs (0.0 = no diversity, 1.0 = max diversity)

gold_set:
  # enable/disable gold set features
  enabled: false  # Disabled to test actual LLM performance
  
  # path to gold JSONL file (relative to backend/)
  gold_file_path: "../automation_testing/gold.jsonl"
  
  # direct answer settings
  enable_direct_answer: true
  direct_answer_threshold: 0.95  # 95% similarity for direct answers

enhancements:
  enabled: false  # master switch for all enhancements - DISABLED: hurts scores by 23%
  
  # query Enhancement
  query_enhancement:
    enabled: false
    expand_acronyms: true
    boost_key_terms: true
    policy_rewriting: true
  
  # re-ranking
  reranking:
    enabled: false
    use_cross_encoder: true  # cross-encoder/ms-marco-MiniLM-L-6-v2
    use_tfidf: true
    use_metadata: true
    max_candidates_for_cross_encoder: 20
    diversity_threshold: 0.7
    
    # score combination weights (should sum to 1.0)
    weights:
      semantic: 0.25      
      cross_encoder: 0.45 
      tfidf: 0.15         
      metadata: 0.15      
  
  # contextual Compression (rule-based)
  compression:
    enabled: false
    deduplicate: true
    aggressive: false
    min_chunk_length: 200
    compression_ratio:
      tier1: 0.8
      tier2: 0.7
      tier3: 0.6
      tier4: 0.5

performance:
  lazy_load_models: true
  cache_size: 128
  max_tokens: 200  # Increased from 128 - allow more complete answers with all nuggets
  use_finetuned_model: false  # Set to true to use fine-tuned model, false for base model
  
  gold_chunk_boost: 2.5
  gold_url_boost: 1.5
  gold_answer_boost: 1.8
  
  # ensure at least one gold chunk in results if relevant
  ensure_gold_in_results: true
  min_gold_score: 0.3
  
  # prefer gold answers when model is uncertain
  prefer_gold_on_uncertainty: true
  
  # semantic matching settings
  semantic_match:
    enabled: true
    threshold: 0.85

# Synthetic Q&A Generation - improves semantic matching without gold set
synthetic_qa:
  enabled: true  # Generate Q&A pairs from catalog chunks using LLM
  boost_synthetic_qa: 1.0  # Slight boost for synthetic Q&A chunks (they match queries better)
  min_chunk_length: 50  # Skip chunks shorter than this
  questions_per_chunk: 3  # Generate multiple questions per chunk for better coverage
  question_model: "google/flan-t5-large" # Model for question generation (only runs during indexing, not live)
  temperature: 0.7  # Higher temperature for diversity across multiple questions
  max_question_length: 60  # Max tokens for generated questions
  force_cpu: true  # Force CPU for generation (set to false to try GPU)
  
  # Tier filtering - which tiers to generate synthetic Q&A for
  # Options: "all" or list of tier numbers [0, 1, 2, 3, 4]
  # Tier 0: Gold set, Tier 1: Academic regs, Tier 2: General, Tier 3: Program-specific, Tier 4: Other programs
  # Example: [1, 2] = only academic regulations and general info (~2000 chunks = ~1hr on CPU)
  # Example: "all" = all chunks (~11000 chunks = ~6-15hrs on CPU)
  # NOTE: With questions_per_chunk=3, generation time ~3x longer
  generate_for_tiers: [1, 2]  # Only generate for important chunks to save time
  
  # Specific programs to generate Q&A for (in addition to tiers above)
  # Leave empty [] to skip program-specific generation
  # Program URLs must match exactly (case-sensitive)
  generate_for_programs:
    - "biotechnology-industrial-biomedical-sciences-ms"
    - "cybersecurity-engineering-ms"
    - "information-technology-ms"

# HyDE (Hypothetical Document Embeddings) - query-time enhancement
# Instead of embedding the query, generate a hypothetical answer and embed that
# This improves retrieval by matching answer-to-answer instead of query-to-answer
# NOTE: This runs at QUERY TIME (every user query), so speed is critical!
hyde:
  enabled: false  # Enable HyDE for query expansion
  model: "google/flan-t5-small"  # SMALL model for speed (runs every query!) - can try "base" if too slow
  force_cpu: true  # Use GPU if available for faster generation (queries need to be fast)
  max_tokens: 100  # Maximum tokens for hypothetical document (reduced for speed)
  temperature: 0.5  # Lower = more focused, higher = more diverse
  min_query_words: 3  # Minimum words in query to use HyDE (skip for very short queries)
  verbose: false  # Set to true to see HyDE generations in logs
  
  # Hybrid mode: combine original query + HyDE results
  hybrid_mode: false  # If true, search with both original query AND HyDE, then merge
  hybrid_weight_original: 0.3  # Weight for original query results (if hybrid_mode=true)
  hybrid_weight_hyde: 0.7  # Weight for HyDE results (if hybrid_mode=true)

# gold set statistics
stats:
  total_entries: 0
  categories: {}
